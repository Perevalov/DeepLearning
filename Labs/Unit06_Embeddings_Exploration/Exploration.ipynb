{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we're going to explore the simple properties of the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\", font_scale=1.3)\n",
    "matplotlib.rcParams[\"legend.framealpha\"] = 1\n",
    "matplotlib.rcParams[\"legend.frameon\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for the sake of reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained embedding can be found [here](https://www.dropbox.com/s/9pu6mt769kj8803/pretrained_embedding.tar.gz?dl=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-10-21 14:21:14--  https://www.dropbox.com/s/9pu6mt769kj8803/pretrained_embedding.tar.gz\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.66.1, 2620:100:6022:1::a27d:4201\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.66.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://dl.dropboxusercontent.com/content_link/qqLtLP5EFuLZd9jJJ3B4mNo2zxKWyvN1x3dSUyiVWdvTRGa9KZAv7SUmgJC39Uey/file [following]\n",
      "--2017-10-21 14:21:15--  https://dl.dropboxusercontent.com/content_link/qqLtLP5EFuLZd9jJJ3B4mNo2zxKWyvN1x3dSUyiVWdvTRGa9KZAv7SUmgJC39Uey/file\n",
      "Resolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 162.125.66.6, 2620:100:6022:6::a27d:4206\n",
      "Connecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|162.125.66.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 866689471 (827M) [application/octet-stream]\n",
      "Saving to: 'pretrained_embedding.tar.gz'\n",
      "\n",
      "pretrained_embeddin 100%[===================>] 826.54M  52.7MB/s    in 17s     \n",
      "\n",
      "2017-10-21 14:21:33 (48.0 MB/s) - 'pretrained_embedding.tar.gz' saved [866689471/866689471]\n",
      "\n",
      "wiki_w2v.vec\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/9pu6mt769kj8803/pretrained_embedding.tar.gz\n",
    "!tar -xvzf pretrained_embedding.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open(\"./wiki_w2v.vec\")\n",
    "f.readline()\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension is 100\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = len(embeddings_index[\"the\"])\n",
    "print(\"Embedding dimension is\", EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of word-vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"the\"\n",
      " [-0.139744    0.20813    -0.118162    0.061692   -0.002767    0.19353899\n",
      " -0.137596   -0.081763    0.10003    -0.023349   -0.35396001 -0.086922\n",
      " -0.10971     0.14912     0.24900199  0.090235   -0.175136    0.100408\n",
      "  0.227097   -0.38284701  0.20857599  0.203307   -0.060234    0.137311\n",
      "  0.100868   -0.005166   -0.06901    -0.29135701 -0.077014    0.077363\n",
      " -0.67388499  0.33003101 -0.32567599 -0.003312   -0.069184   -0.016198\n",
      " -0.20462    -0.01192     0.105787    0.039009    0.38602901  0.014063\n",
      " -0.34556401 -0.185229   -0.19212601 -0.051685   -0.055161   -0.279562\n",
      " -0.31859699 -0.17527901  0.223395   -0.40426299  0.030896   -0.27991199\n",
      " -0.018625   -0.072526    0.0099      0.31887901  0.26463899 -0.157149\n",
      " -0.41845399  0.030546    0.30279899  0.126863    0.057413   -0.278595\n",
      " -0.49134201 -0.37145299  0.37209201 -0.18141     0.34714001  0.076957\n",
      " -0.20196401 -0.257523    0.095072    0.019826   -0.176595    0.39072999\n",
      "  0.165888    0.17529801 -0.37434199 -0.088488    0.097903   -0.004904\n",
      " -0.224013   -0.02797     0.05367    -0.081368    0.249916    0.184791\n",
      " -0.61333501 -0.32723299 -0.33728999  0.38315001  0.123009   -0.25866601\n",
      "  0.15735     0.057887    0.116886    0.0856    ]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\\"the\\\"\\n\", embeddings_index[\"the\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closest word (Quiz 6.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement simple class to find the closest word to the given one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Embeddings(object):\n",
    "    \n",
    "    def __init__(self, embedding_dict):\n",
    "        \"\"\"\n",
    "        Implements simple routine to work with word vector\n",
    "        using pre-trained embedding and sklearn nearest\n",
    "        neighbors module\n",
    "        \n",
    "        Args:\n",
    "            embedding_dict(dict): words as keys and vectors as values\n",
    "\n",
    "        Return:\n",
    "            self\n",
    "        \"\"\"\n",
    "        # initialize word and vector arrays\n",
    "        self.words = np.array(list(embedding_dict.keys()))\n",
    "        self.vectors = np.array(list(embedding_dict.values()))\n",
    "        # fir nearest neighbors model with cosine similatiry between word-vectors\n",
    "        self._nn = NearestNeighbors(metric=\"cosine\", n_jobs=1, algorithm=\"brute\")\n",
    "        self._nn.fit(self.vectors)\n",
    "        \n",
    "    def get_vector(self, word):\n",
    "        \"\"\"\n",
    "        Return the word-vector for given word if it is in the vocabulary\n",
    "        \n",
    "        Args:\n",
    "            word(str): word to look up for\n",
    "            \n",
    "        Return:\n",
    "            vector(ndarray): vector for given word\n",
    "        \"\"\"\n",
    "        \n",
    "        assert (word in self.words), \"Word is not in the vocabulary\"\n",
    "        word_idx = np.where(self.words == word)[0]\n",
    "        return self.vectors[word_idx]\n",
    "            \n",
    "    def closest(self, word, n_closest=1, include_itself=False):\n",
    "        \"\"\"\n",
    "        Find the closest word in the vocabulary to the given data\n",
    "        \n",
    "        Args:\n",
    "            word(str or ndarray): if str then the corresponding\n",
    "                                  vector will be retrieved first;\n",
    "                                  ndarray will be treated as a\n",
    "                                  word-vector\n",
    "            n_closest(int): number of closest words to return\n",
    "            include_itself(bool): whether to include the vector\n",
    "                                  itself if it is in the vocabulary\n",
    "                                  \n",
    "        Return:\n",
    "            words(ndarray of str): array with closest words\n",
    "        \"\"\"\n",
    "        # check which input do we have\n",
    "        if type(word) is str:\n",
    "            word_vector = self.get_vector(word)\n",
    "        else:\n",
    "            word_vector = word\n",
    "        # distinguish between returning itself and not\n",
    "        if include_itself:\n",
    "            closest_idxs = self._nn.kneighbors(word_vector, n_neighbors=n_closest)\n",
    "            return self.words[closest_idxs[1][0]]\n",
    "        else:\n",
    "            closest_idxs = self._nn.kneighbors(word_vector, n_neighbors=n_closest + 1)\n",
    "            return self.words[closest_idxs[1][0][1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v = Embeddings(embeddings_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the closest word (*Quiz 6.1*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['allay'],\n",
       "      dtype='<U110')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.closest(\"assuage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analogical reasoning (Quiz 6.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to perform a task of analogical reasoning (for the definition see lectures or [original paper](https://arxiv.org/pdf/1301.3781.pdf))\n",
    "\n",
    "Here we want to explore semantic relations, e.g., **Country-Capital** (*Quiz 6.2*)\n",
    "\n",
    "The question is what relates to Moscow the same as Ireland relates to Dublin? To find the answer to this question remember that our embedding space has the property of linearity. Thus let's take the vector for each word and perform simple arithmetic operations on them, i.e.,\n",
    "\n",
    "$$\n",
    "w[\\text{ireland}] - w[\\text{dublin}] + w[\\text{moscow}] = X\n",
    "$$\n",
    "\n",
    "and then find the vector in our embedding space which is closest to the resulting $X$ vector. Hopefully it will be Russia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = w2v.get_vector(\"ireland\") - w2v.get_vector(\"dublin\") + w2v.get_vector(\"moscow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['russia', 'ukraine', 'belarus'],\n",
       "      dtype='<U110')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.closest(X, include_itself=True, n_closest=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
